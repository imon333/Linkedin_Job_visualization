{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install selenium geopy pandas dash plotly webdriver-manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, dash_table\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from geopy.geocoders import Nominatim\n",
    "import io\n",
    "import base64\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure WebDriver for Chrome browser (headless)\n",
    "def configure_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create Helper Function to Calculate \"Days Ago\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate days ago\n",
    "def days_ago(posted_time):\n",
    "    now = datetime.now()\n",
    "    if 'hour' in posted_time:\n",
    "        return 0  # Posted today\n",
    "    match = re.search(r'(\\d+)\\s*(days?|weeks?|months?|years?)\\s*ago', posted_time)\n",
    "    if match:\n",
    "        number = int(match.group(1))\n",
    "        unit = match.group(2)\n",
    "        \n",
    "        if 'day' in unit:\n",
    "            delta = timedelta(days=number)\n",
    "        elif 'week' in unit:\n",
    "            delta = timedelta(weeks=number)\n",
    "        elif 'month' in unit:\n",
    "            delta = timedelta(weeks=number * 4)\n",
    "        elif 'year' in unit:\n",
    "            delta = timedelta(weeks=number * 52)\n",
    "        \n",
    "        posted_date = now - delta\n",
    "        return (now - posted_date).days  # Return number of days ago\n",
    "    return 0  # Default if no match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping LinkedIn Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape LinkedIn for jobs\n",
    "def scrape_linkedin_jobs(keyword, location):\n",
    "    print(\"\\nüîç Scraping LinkedIn Jobs...\\n\")\n",
    "    driver = configure_driver()\n",
    "    if not driver:\n",
    "        return []\n",
    "    try:\n",
    "        search_url = f\"https://www.linkedin.com/jobs/search?keywords={keyword.replace(' ', '%20')}&location={location.replace(' ', '%20')}\"\n",
    "        driver.get(search_url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        for _ in range(3):  # Scroll 3 times\n",
    "            driver.execute_script(\"window.scrollBy(0, 800);\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"base-card\")))\n",
    "        \n",
    "        jobs = []\n",
    "        job_elements = driver.find_elements(By.CLASS_NAME, \"base-card\")\n",
    "        \n",
    "        for job in job_elements[:20]:  # Limit to first 20 jobs\n",
    "            try:\n",
    "                title = job.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
    "                company = job.find_element(By.CSS_SELECTOR, \"h4\").text.strip()\n",
    "                location = job.find_element(By.CLASS_NAME, \"job-search-card__location\").text.strip()\n",
    "                link = job.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                posted_time = job.find_element(By.CLASS_NAME, \"job-search-card__listdate\").text.strip()\n",
    "                \n",
    "                # Get days ago posted\n",
    "                days_posted = days_ago(posted_time)\n",
    "                \n",
    "                jobs.append({\"title\": title, \"company\": company, \"location\": location, \"link\": link, \"days_ago_posted\": days_posted})\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping job: {e}\")\n",
    "                continue\n",
    "        return jobs\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Location Data (Geocoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and add geolocation data (latitude/longitude)\n",
    "def clean_location_data(df):\n",
    "    geolocator = Nominatim(user_agent=\"job_scraper\")\n",
    "    df[['City', 'Country']] = df['location'].str.rsplit(',', n=1, expand=True)\n",
    "    df['Country'] = df['Country'].str.strip()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df['Latitude'] = None\n",
    "    df['Longitude'] = None\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            location = geolocator.geocode(row['location'])\n",
    "            if location:\n",
    "                df.at[index, 'Latitude'] = location.latitude\n",
    "                df.at[index, 'Longitude'] = location.longitude\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Jobs to Excel and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save jobs to Excel first, then clean and save as CSV\n",
    "def save_jobs_to_file(jobs, keyword, location):\n",
    "    if not jobs:\n",
    "        print(\"No jobs found.\")\n",
    "        return None\n",
    "    df = pd.DataFrame(jobs)\n",
    "    \n",
    "    today_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    filename = f\"linkedin_jobs_{keyword.replace(' ', '_')}_{location.replace(' ', '_')}_{today_date}\"\n",
    "    excel_file = f\"{filename}.xlsx\"\n",
    "    df.to_excel(excel_file, index=False)\n",
    "    print(f\"Saved data to {excel_file}\")\n",
    "    \n",
    "    # Now clean and save as CSV\n",
    "    df = clean_location_data(df)\n",
    "    csv_file = f\"{filename}.csv\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Cleaned data saved to {csv_file}\")\n",
    "    return csv_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dash App for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Scraping LinkedIn Jobs...\n",
      "\n",
      "Error scraping job: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".job-search-card__listdate\"}\n",
      "  (Session info: chrome=134.0.6998.89); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100e01804 cxxbridge1$str$ptr + 2785964\n",
      "1   chromedriver                        0x0000000100df9ddc cxxbridge1$str$ptr + 2754692\n",
      "2   chromedriver                        0x000000010094dea8 cxxbridge1$string$len + 92928\n",
      "3   chromedriver                        0x00000001009951d0 cxxbridge1$string$len + 384552\n",
      "4   chromedriver                        0x000000010098a878 cxxbridge1$string$len + 341200\n",
      "5   chromedriver                        0x00000001009d6678 cxxbridge1$string$len + 651984\n",
      "6   chromedriver                        0x000000010098935c cxxbridge1$string$len + 335796\n",
      "7   chromedriver                        0x0000000100dc6cd4 cxxbridge1$str$ptr + 2545532\n",
      "8   chromedriver                        0x0000000100dc9fa0 cxxbridge1$str$ptr + 2558536\n",
      "9   chromedriver                        0x0000000100da6d04 cxxbridge1$str$ptr + 2414508\n",
      "10  chromedriver                        0x0000000100dca800 cxxbridge1$str$ptr + 2560680\n",
      "11  chromedriver                        0x0000000100d97ba0 cxxbridge1$str$ptr + 2352712\n",
      "12  chromedriver                        0x0000000100dea45c cxxbridge1$str$ptr + 2690820\n",
      "13  chromedriver                        0x0000000100dea5e4 cxxbridge1$str$ptr + 2691212\n",
      "14  chromedriver                        0x0000000100df9a50 cxxbridge1$str$ptr + 2753784\n",
      "15  libsystem_pthread.dylib             0x0000000180e672e4 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000180e620fc thread_start + 8\n",
      "\n",
      "Saved data to linkedin_jobs_working_student_hamburg_2025-03-13.xlsx\n",
      "Cleaned data saved to linkedin_jobs_working_student_hamburg_2025-03-13.csv\n",
      "CSV saved: linkedin_jobs_working_student_hamburg_2025-03-13.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the script\n",
    "keyword = input(\"Enter job title: \")\n",
    "location = input(\"Enter location: \")\n",
    "jobs = scrape_linkedin_jobs(keyword, location)\n",
    "csv_file = save_jobs_to_file(jobs, keyword, location)\n",
    "if csv_file:\n",
    "    print(f\"CSV saved: {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV_R",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
